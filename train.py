import json
import os
import argparse
import pickle
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.optimizers import Adam
from utils.preprocessing import get_preprocessed_data
from models.architectures import get_model

def plot_history(history, save_path):
    """
    í•™ìŠµ ê³¼ì •ì˜ Loss ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()
    print(f"  - Loss plot saved to {save_path}")

def main(args):
    # 1. Config ë¡œë“œ
    config_path = 'configs/config.json'
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found at {config_path}")
        
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # CLI ì¸ìë¡œ Config ë®ì–´ì“°ê¸° (ì‹¤í—˜ ì‹œ ìœ ìš©)
    model_name = args.model if args.model else config['target_models'][0]
    epochs = args.epochs if args.epochs else config['epochs']
    batch_size = args.batch_size if args.batch_size else config['batch_size']
    learning_rate = args.lr if args.lr else config['learning_rate']
    
    print(f"\nğŸš€ Start Training Configuration:")
    print(f"  - Model: {model_name}")
    print(f"  - Epochs: {epochs}")
    print(f"  - Batch Size: {batch_size}")
    print(f"  - Learning Rate: {learning_rate}")
    print(f"  - Window Size: {config['window_size']}")
    print("-" * 40)

    # 2. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # get_preprocessed_dataëŠ” (train, val, test1, test2) 4ê°œì˜ í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” trainê³¼ valë§Œ í•„ìš”í•˜ë¯€ë¡œ ë‚˜ë¨¸ì§€ëŠ” _ë¡œ ë°›ìŠµë‹ˆë‹¤.
    print("\nğŸ”„ Running Data Preprocessing Pipeline...")
    train_tensor, val_tensor, _, _ = get_preprocessed_data(config)
    
    print(f"  - Train Tensor Shape: {train_tensor.shape}")
    print(f"  - Val Tensor Shape: {val_tensor.shape}")

    # 3. ëª¨ë¸ ë¹Œë“œ
    # ì…ë ¥ í˜•íƒœ: (Window Size, Feature Dimension)
    input_shape = (train_tensor.shape[1], train_tensor.shape[2])
    model = get_model(model_name, input_shape)
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    
    print(f"\nğŸ—ï¸ Model Architecture: {model_name}")
    model.summary()

    # 4. ì²´í¬í¬ì¸íŠ¸ ë° ì½œë°± ì„¤ì •
    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(config['model_checkpoint_dir']):
        os.makedirs(config['model_checkpoint_dir'])
    
    if not os.path.exists(config['output_dir']):
        os.makedirs(config['output_dir'])
        
    checkpoint_path = os.path.join(config['model_checkpoint_dir'], f"{model_name}_best.h5")
    
    callbacks = [
        ModelCheckpoint(
            filepath=checkpoint_path,
            monitor='val_loss',
            verbose=1,
            save_best_only=True,
            mode='min'
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            verbose=1,
            mode='min',
            restore_best_weights=True
        )
    ]

    # 5. í•™ìŠµ ì‹¤í–‰
    print("\nğŸ”¥ Starting Training...")
    history = model.fit(
        train_tensor, train_tensor,  # AutoEncoderëŠ” ì…ë ¥ == íƒ€ê²Ÿ
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(val_tensor, val_tensor),
        callbacks=callbacks,
        shuffle=True
    )
    
    # 6. ê²°ê³¼ ì €ì¥
    # í•™ìŠµ History(Loss ê°’ ë“±)ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥
    history_path = os.path.join(config['output_dir'], f"{model_name}_history.pkl")
    with open(history_path, 'wb') as f:
        pickle.dump(history.history, f)
        
    # Loss ê·¸ë˜í”„ ì €ì¥
    plot_path = os.path.join(config['output_dir'], f"{model_name}_loss.png")
    plot_history(history, plot_path)
    
    print(f"\nâœ… Training Finished Successfully.")
    print(f"  - Best Model saved at: {checkpoint_path}")
    print(f"  - History saved at: {history_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Anomaly Detection Models on HAI Dataset")
    
    parser.add_argument("--model", type=str, default=None, 
                        help="Name of the model to train (e.g., Conv_BiLSTM_AE, Conv_BiLSTM_AE_Attention)")
    parser.add_argument("--epochs", type=int, default=None, 
                        help="Number of epochs to train")
    parser.add_argument("--batch_size", type=int, default=None, 
                        help="Batch size for training")
    parser.add_argument("--lr", type=float, default=None, 
                        help="Learning rate")
    
    args = parser.parse_args()
    
    # GPU ì„¤ì • í™•ì¸ (ë©”ëª¨ë¦¬ ì¦ê°€ í• ë‹¹)
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"âœ… GPU Available: {len(gpus)} device(s)")
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
    else:
        print("âš ï¸ GPU NOT Available. Training will use CPU.")

    main(args)